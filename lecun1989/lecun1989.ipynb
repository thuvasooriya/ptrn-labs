{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first ever cnn model\n",
    "this is the first ever cnn model proposed by yann lecun in 1989. this is a simple model with 2 convolutional layers and 2 fully connected layers.\n",
    "this notebook tries to implement the model using pytorch.\n",
    "\n",
    "[paper](https://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf)\n",
    "\n",
    "credits\n",
    "1. [karpathy](https://github.com/karpathy/lecun1989-repro)\n",
    "2. [article about the reference repo](https://karpathy.github.io/2022/03/14/lecun1989/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n",
      "cuda device not found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print(\"mps device not found.\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cuda_device = torch.device(\"cuda\")\n",
    "    x = torch.ones(1, device=cuda_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print(\"cuda device not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "preprocess today's mnist dataset into 1989 version's size/format (approximately)\n",
    "http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf\n",
    "\n",
    "some relevant notes for this part:\n",
    "- 7291 digits are used for training\n",
    "- 2007 digits are used for testing\n",
    "- each image is 16x16 pixels grayscale (not binary)\n",
    "- images are scaled to range [-1, 1]\n",
    "- paper doesn't say exactly, but reading between the lines i assume label targets to be {-1, 1}\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "np.random.seed(1337)\n",
    "\n",
    "for split in {\"train\", \"test\"}:\n",
    "    data = datasets.MNIST(\"./data\", train=split == \"train\", download=True)\n",
    "\n",
    "    n = 7291 if split == \"train\" else 2007\n",
    "    rp = np.random.permutation(len(data))[:n]\n",
    "\n",
    "    X = torch.full((n, 1, 16, 16), 0.0, dtype=torch.float32)\n",
    "    Y = torch.full((n, 10), -1.0, dtype=torch.float32)\n",
    "    for i, ix in enumerate(rp):\n",
    "        I, yint = data[int(ix)]\n",
    "        # PIL image -> numpy -> torch tensor -> [-1, 1] fp32\n",
    "        xi = torch.from_numpy(np.array(I, dtype=np.float32)) / 127.5 - 1.0\n",
    "        # add a fake batch dimension and a channel dimension of 1 or F.interpolate won't be happy\n",
    "        xi = xi[None, None, ...]\n",
    "        # resize to (16, 16) images with bilinear interpolation\n",
    "        xi = F.interpolate(xi, (16, 16), mode=\"bilinear\")\n",
    "        X[i] = xi[0]  # store\n",
    "\n",
    "        # set the correct class to have target of +1.0\n",
    "        Y[i, yint] = 1.0\n",
    "\n",
    "    torch.save((X, Y), split + \"1989.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model stats:\n",
      "# params:       9760\n",
      "# MACs:         63660\n",
      "# activations:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/pvqqyvg55vdb3xz5tmq6gc2h0000gn/T/ipykernel_64116/2161314919.py:155: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Xtr, Ytr = torch.load(\"train1989.pt\")\n",
      "/var/folders/2j/pvqqyvg55vdb3xz5tmq6gc2h0000gn/T/ipykernel_64116/2161314919.py:156: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Xte, Yte = torch.load(\"test1989.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "eval: split train. loss 6.522415e-02. error 10.15%. misses: 739\n",
      "eval: split test . loss 6.352933e-02. error 9.87%. misses: 198\n",
      "2\n",
      "eval: split train. loss 4.566194e-02. error 7.02%. misses: 511\n",
      "eval: split test . loss 4.721165e-02. error 7.37%. misses: 148\n",
      "3\n",
      "eval: split train. loss 3.546033e-02. error 5.27%. misses: 384\n",
      "eval: split test . loss 4.091607e-02. error 6.43%. misses: 128\n",
      "4\n",
      "eval: split train. loss 2.963147e-02. error 4.43%. misses: 322\n",
      "eval: split test . loss 3.777764e-02. error 5.73%. misses: 115\n",
      "5\n",
      "eval: split train. loss 2.526288e-02. error 3.52%. misses: 256\n",
      "eval: split test . loss 3.455522e-02. error 5.38%. misses: 107\n",
      "6\n",
      "eval: split train. loss 2.306855e-02. error 3.42%. misses: 249\n",
      "eval: split test . loss 3.409069e-02. error 4.68%. misses: 94\n",
      "7\n",
      "eval: split train. loss 2.055848e-02. error 3.03%. misses: 220\n",
      "eval: split test . loss 3.238904e-02. error 4.43%. misses: 89\n",
      "8\n",
      "eval: split train. loss 1.880632e-02. error 2.65%. misses: 192\n",
      "eval: split test . loss 3.244922e-02. error 4.73%. misses: 94\n",
      "9\n",
      "eval: split train. loss 1.769457e-02. error 2.44%. misses: 177\n",
      "eval: split test . loss 3.173990e-02. error 4.58%. misses: 92\n",
      "10\n",
      "eval: split train. loss 1.706421e-02. error 2.44%. misses: 177\n",
      "eval: split test . loss 3.092523e-02. error 4.48%. misses: 89\n",
      "11\n",
      "eval: split train. loss 1.410019e-02. error 1.98%. misses: 143\n",
      "eval: split test . loss 3.054956e-02. error 4.29%. misses: 86\n",
      "12\n",
      "eval: split train. loss 1.322742e-02. error 1.76%. misses: 127\n",
      "eval: split test . loss 2.950036e-02. error 4.29%. misses: 86\n",
      "13\n",
      "eval: split train. loss 1.212914e-02. error 1.65%. misses: 119\n",
      "eval: split test . loss 3.122100e-02. error 4.63%. misses: 92\n",
      "14\n",
      "eval: split train. loss 1.218602e-02. error 1.73%. misses: 126\n",
      "eval: split test . loss 3.107965e-02. error 4.38%. misses: 87\n",
      "15\n",
      "eval: split train. loss 1.105863e-02. error 1.44%. misses: 104\n",
      "eval: split test . loss 3.097889e-02. error 4.48%. misses: 89\n",
      "16\n",
      "eval: split train. loss 9.789606e-03. error 1.33%. misses: 96\n",
      "eval: split test . loss 3.110767e-02. error 4.29%. misses: 86\n",
      "17\n",
      "eval: split train. loss 8.869461e-03. error 1.28%. misses: 92\n",
      "eval: split test . loss 3.072087e-02. error 4.78%. misses: 95\n",
      "18\n",
      "eval: split train. loss 8.285410e-03. error 1.17%. misses: 84\n",
      "eval: split test . loss 2.952643e-02. error 4.04%. misses: 81\n",
      "19\n",
      "eval: split train. loss 7.082038e-03. error 1.08%. misses: 78\n",
      "eval: split test . loss 2.783582e-02. error 3.74%. misses: 74\n",
      "20\n",
      "eval: split train. loss 6.785025e-03. error 1.03%. misses: 74\n",
      "eval: split test . loss 2.805937e-02. error 3.89%. misses: 77\n",
      "21\n",
      "eval: split train. loss 6.019350e-03. error 0.92%. misses: 67\n",
      "eval: split test . loss 2.770438e-02. error 3.74%. misses: 74\n",
      "22\n",
      "eval: split train. loss 5.739770e-03. error 0.89%. misses: 65\n",
      "eval: split test . loss 2.753974e-02. error 3.64%. misses: 72\n",
      "23\n",
      "eval: split train. loss 5.382472e-03. error 0.88%. misses: 63\n",
      "eval: split test . loss 2.776756e-02. error 3.69%. misses: 74\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Running this script eventually gives:\n",
    "23\n",
    "eval: split train. loss 4.073383e-03. error 0.62%. misses: 45\n",
    "eval: split test . loss 2.838382e-02. error 4.09%. misses: 82\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter  # pip install tensorboardX\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"1989 LeCun ConvNet per description in the paper\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialization as described in the paper to my best ability, but it doesn't look right...\n",
    "        winit = (\n",
    "            lambda fan_in, *shape: (torch.rand(*shape) - 0.5) * 2 * 2.4 / fan_in**0.5\n",
    "        )\n",
    "        macs = 0  # keep track of MACs (multiply accumulates)\n",
    "        acts = 0  # keep track of number of activations\n",
    "\n",
    "        # H1 layer parameters and their initialization\n",
    "        self.H1w = nn.Parameter(winit(5 * 5 * 1, 12, 1, 5, 5))\n",
    "        self.H1b = nn.Parameter(\n",
    "            torch.zeros(12, 8, 8)\n",
    "        )  # presumably init to zero for biases\n",
    "        assert self.H1w.nelement() + self.H1b.nelement() == 1068\n",
    "        macs += (5 * 5 * 1) * (8 * 8) * 12\n",
    "        acts += (8 * 8) * 12\n",
    "\n",
    "        # H2 layer parameters and their initialization\n",
    "        \"\"\"\n",
    "        H2 neurons all connect to only 8 of the 12 input planes, with an unspecified pattern\n",
    "        I am going to assume the most sensible block pattern where 4 planes at a time connect\n",
    "        to differently overlapping groups of 8/12 input planes. We will implement this with 3\n",
    "        separate convolutions that we concatenate the results of.\n",
    "        \"\"\"\n",
    "        self.H2w = nn.Parameter(winit(5 * 5 * 8, 12, 8, 5, 5))\n",
    "        self.H2b = nn.Parameter(\n",
    "            torch.zeros(12, 4, 4)\n",
    "        )  # presumably init to zero for biases\n",
    "        assert self.H2w.nelement() + self.H2b.nelement() == 2592\n",
    "        macs += (5 * 5 * 8) * (4 * 4) * 12\n",
    "        acts += (4 * 4) * 12\n",
    "\n",
    "        # H3 is a fully connected layer\n",
    "        self.H3w = nn.Parameter(winit(4 * 4 * 12, 4 * 4 * 12, 30))\n",
    "        self.H3b = nn.Parameter(torch.zeros(30))\n",
    "        assert self.H3w.nelement() + self.H3b.nelement() == 5790\n",
    "        macs += (4 * 4 * 12) * 30\n",
    "        acts += 30\n",
    "\n",
    "        # output layer is also fully connected layer\n",
    "        self.outw = nn.Parameter(winit(30, 30, 10))\n",
    "        self.outb = nn.Parameter(\n",
    "            -torch.ones(10)\n",
    "        )  # 9/10 targets are -1, so makes sense to init slightly towards it\n",
    "        assert self.outw.nelement() + self.outb.nelement() == 310\n",
    "        macs += 30 * 10\n",
    "        acts += 10\n",
    "\n",
    "        self.macs = macs\n",
    "        self.acts = acts\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x has shape (1, 1, 16, 16)\n",
    "        x = F.pad(\n",
    "            x, (2, 2, 2, 2), \"constant\", -1.0\n",
    "        )  # pad by two using constant -1 for background\n",
    "        x = F.conv2d(x, self.H1w, stride=2) + self.H1b\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        # x is now shape (1, 12, 8, 8)\n",
    "        x = F.pad(\n",
    "            x, (2, 2, 2, 2), \"constant\", -1.0\n",
    "        )  # pad by two using constant -1 for background\n",
    "        slice1 = F.conv2d(\n",
    "            x[:, 0:8], self.H2w[0:4], stride=2\n",
    "        )  # first 4 planes look at first 8 input planes\n",
    "        slice2 = F.conv2d(\n",
    "            x[:, 4:12], self.H2w[4:8], stride=2\n",
    "        )  # next 4 planes look at last 8 input planes\n",
    "        slice3 = F.conv2d(\n",
    "            torch.cat((x[:, 0:4], x[:, 8:12]), dim=1), self.H2w[8:12], stride=2\n",
    "        )  # last 4 planes are cross\n",
    "        x = torch.cat((slice1, slice2, slice3), dim=1) + self.H2b\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        # x is now shape (1, 12, 4, 4)\n",
    "        x = x.flatten(start_dim=1)  # (1, 12*4*4)\n",
    "        x = x @ self.H3w + self.H3b\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        # x is now shape (1, 30)\n",
    "        x = x @ self.outw + self.outb\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        # x is finally shape (1, 10)\n",
    "        return x\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser(description=\"Train a 1989 LeCun ConvNet on digits\")\n",
    "    # parser.add_argument(\n",
    "    #     \"--learning-rate\", \"-l\", type=float, default=0.03, help=\"SGD learning rate\"\n",
    "    # )\n",
    "    # parser.add_argument(\n",
    "    #     \"--output-dir\",\n",
    "    #     \"-o\",\n",
    "    #     type=str,\n",
    "    #     default=\"out/base\",\n",
    "    #     help=\"output directory for training logs\",\n",
    "    # )\n",
    "    # args = parser.parse_args()\n",
    "    # print(vars(args))\n",
    "    learning_rate = 0.03\n",
    "    output_dir = \"out/base\"\n",
    "\n",
    "    # init rng\n",
    "    torch.manual_seed(1337)\n",
    "    np.random.seed(1337)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    # set up logging\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # with open(os.path.join(output_dir, \"args.json\"), \"w\") as f:\n",
    "    #     json.dump(vars(args), f, indent=2)\n",
    "    writer = SummaryWriter(output_dir)\n",
    "\n",
    "    # init a model\n",
    "    model = Net()\n",
    "    print(\"model stats:\")\n",
    "    print(\n",
    "        \"# params:      \", sum(p.numel() for p in model.parameters())\n",
    "    )  # in paper total is 9,760\n",
    "    print(\"# MACs:        \", model.macs)\n",
    "    print(\"# activations: \", model.acts)\n",
    "\n",
    "    # init data\n",
    "    Xtr, Ytr = torch.load(\"train1989.pt\")\n",
    "    Xte, Yte = torch.load(\"test1989.pt\")\n",
    "\n",
    "    # init optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def eval_split(split):\n",
    "        # eval the full train/test set, batched implementation for efficiency\n",
    "        model.eval()\n",
    "        X, Y = (Xtr, Ytr) if split == \"train\" else (Xte, Yte)\n",
    "        Yhat = model(X)\n",
    "        loss = torch.mean((Y - Yhat) ** 2)\n",
    "        err = torch.mean((Y.argmax(dim=1) != Yhat.argmax(dim=1)).float())\n",
    "        print(\n",
    "            f\"eval: split {split:5s}. loss {loss.item():e}. error {err.item()*100:.2f}%. misses: {int(err.item()*Y.size(0))}\"\n",
    "        )\n",
    "        writer.add_scalar(f\"error/{split}\", err.item() * 100, pass_num)\n",
    "        writer.add_scalar(f\"loss/{split}\", loss.item(), pass_num)\n",
    "\n",
    "    # train\n",
    "    for pass_num in range(23):\n",
    "        # perform one epoch of training\n",
    "        model.train()\n",
    "        for step_num in range(Xtr.size(0)):\n",
    "            # fetch a single example into a batch of 1\n",
    "            x, y = Xtr[[step_num]], Ytr[[step_num]]\n",
    "\n",
    "            # forward the model and the loss\n",
    "            yhat = model(x)\n",
    "            loss = torch.mean((y - yhat) ** 2)\n",
    "\n",
    "            # calculate the gradient and update the parameters\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # after epoch epoch evaluate the train and test error / metrics\n",
    "        print(pass_num + 1)\n",
    "        eval_split(\"train\")\n",
    "        eval_split(\"test\")\n",
    "\n",
    "    # save final model to file\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model stats:\n",
      "# params:       9760\n",
      "# MACs:         63660\n",
      "# activations:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/pvqqyvg55vdb3xz5tmq6gc2h0000gn/T/ipykernel_69325/2767757738.py:195: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Xtr, Ytr = torch.load(\"train1989.pt\")\n",
      "/var/folders/2j/pvqqyvg55vdb3xz5tmq6gc2h0000gn/T/ipykernel_69325/2767757738.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Xte, Yte = torch.load(\"test1989.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "eval: split train. loss 1.198310e-01. error 16.14%. misses: 1177\n",
      "eval: split test . loss 1.198310e-01. error 14.95%. misses: 299\n",
      "2\n",
      "eval: split train. loss 8.156618e-02. error 9.23%. misses: 673\n",
      "eval: split test . loss 8.156618e-02. error 8.57%. misses: 172\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 233\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# calculate the gradient and update the parameters\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 233\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# after epoch epoch evaluate the train and test error / metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/arc/pro/dev/pydev/ptrn-labs/.venv/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/arc/pro/dev/pydev/ptrn-labs/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/arc/pro/dev/pydev/ptrn-labs/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "repro.py gives:\n",
    "23\n",
    "eval: split train. loss 4.073383e-03. error 0.62%. misses: 45\n",
    "eval: split test . loss 2.838382e-02. error 4.09%. misses: 82\n",
    "\n",
    "we can try to use our knowledge from 33 years later to improve on this,\n",
    "but keeping the model size same.\n",
    "\n",
    "Change 1: replace tanh on last layer with FC and use softmax. Had to\n",
    "lower the learning rate to 0.01 as well. This improves the optimization\n",
    "quite a lot, we now crush the training set:\n",
    "23\n",
    "eval: split train. loss 9.536698e-06. error 0.00%. misses: 0\n",
    "eval: split test . loss 9.536698e-06. error 4.38%. misses: 87\n",
    "\n",
    "Change 2: change from SGD to AdamW with LR 3e-4 because I find this\n",
    "to be significantly more stable and requires little to no tuning. Also\n",
    "double epochs to 46. I decay the LR to 1e-4 over course of training.\n",
    "These changes make it so optimization is not culprit of bad performance\n",
    "with high probability. We also seem to improve test set a bit:\n",
    "46\n",
    "eval: split train. loss 0.000000e+00. error 0.00%. misses: 0\n",
    "eval: split test . loss 0.000000e+00. error 3.59%. misses: 72\n",
    "\n",
    "Change 3: since we are overfitting we can introduce data augmentation,\n",
    "e.g. let's intro a shift by at most 1 pixel in both x/y directions. Also\n",
    "because we are augmenting we again want to bump up training time, e.g.\n",
    "to 60 epochs:\n",
    "60\n",
    "eval: split train. loss 8.780676e-04. error 1.70%. misses: 123\n",
    "eval: split test . loss 8.780676e-04. error 2.19%. misses: 43\n",
    "\n",
    "Change 4: we want to add dropout at the layer with most parameters (H3),\n",
    "but in addition we also have to shift the activation function to relu so\n",
    "that dropout makes sense. We also bring up iterations to 80:\n",
    "80\n",
    "eval: split train. loss 2.601336e-03. error 1.47%. misses: 106\n",
    "eval: split test . loss 2.601336e-03. error 1.59%. misses: 32\n",
    "\n",
    "To be continued...\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter  # pip install tensorboardX\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"1989 LeCun ConvNet per description in the paper\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialization as described in the paper to my best ability, but it doesn't look right...\n",
    "        winit = (\n",
    "            lambda fan_in, *shape: (torch.rand(*shape) - 0.5) * 2 * 2.4 / fan_in**0.5\n",
    "        )\n",
    "        macs = 0  # keep track of MACs (multiply accumulates)\n",
    "        acts = 0  # keep track of number of activations\n",
    "\n",
    "        # H1 layer parameters and their initialization\n",
    "        self.H1w = nn.Parameter(winit(5 * 5 * 1, 12, 1, 5, 5))\n",
    "        self.H1b = nn.Parameter(\n",
    "            torch.zeros(12, 8, 8)\n",
    "        )  # presumably init to zero for biases\n",
    "        macs += (5 * 5 * 1) * (8 * 8) * 12\n",
    "        acts += (8 * 8) * 12\n",
    "\n",
    "        # H2 layer parameters and their initialization\n",
    "        \"\"\"\n",
    "        H2 neurons all connect to only 8 of the 12 input planes, with an unspecified pattern\n",
    "        I am going to assume the most sensible block pattern where 4 planes at a time connect\n",
    "        to differently overlapping groups of 8/12 input planes. We will implement this with 3\n",
    "        separate convolutions that we concatenate the results of.\n",
    "        \"\"\"\n",
    "        self.H2w = nn.Parameter(winit(5 * 5 * 8, 12, 8, 5, 5))\n",
    "        self.H2b = nn.Parameter(\n",
    "            torch.zeros(12, 4, 4)\n",
    "        )  # presumably init to zero for biases\n",
    "        macs += (5 * 5 * 8) * (4 * 4) * 12\n",
    "        acts += (4 * 4) * 12\n",
    "\n",
    "        # H3 is a fully connected layer\n",
    "        self.H3w = nn.Parameter(winit(4 * 4 * 12, 4 * 4 * 12, 30))\n",
    "        self.H3b = nn.Parameter(torch.zeros(30))\n",
    "        macs += (4 * 4 * 12) * 30\n",
    "        acts += 30\n",
    "\n",
    "        # output layer is also fully connected layer\n",
    "        self.outw = nn.Parameter(winit(30, 30, 10))\n",
    "        self.outb = nn.Parameter(torch.zeros(10))\n",
    "        macs += 30 * 10\n",
    "        acts += 10\n",
    "\n",
    "        self.macs = macs\n",
    "        self.acts = acts\n",
    "\n",
    "    def forward(self, x):\n",
    "        # poor man's data augmentation by 1 pixel along x/y directions\n",
    "        if self.training:\n",
    "            shift_x, shift_y = np.random.randint(-1, 2, size=2)\n",
    "            x = torch.roll(x, (shift_x, shift_y), (2, 3))\n",
    "\n",
    "        # x has shape (1, 1, 16, 16)\n",
    "        x = F.pad(\n",
    "            x, (2, 2, 2, 2), \"constant\", -1.0\n",
    "        )  # pad by two using constant -1 for background\n",
    "        x = F.conv2d(x, self.H1w, stride=2) + self.H1b\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # x is now shape (1, 12, 8, 8)\n",
    "        x = F.pad(\n",
    "            x, (2, 2, 2, 2), \"constant\", -1.0\n",
    "        )  # pad by two using constant -1 for background\n",
    "        slice1 = F.conv2d(\n",
    "            x[:, 0:8], self.H2w[0:4], stride=2\n",
    "        )  # first 4 planes look at first 8 input planes\n",
    "        slice2 = F.conv2d(\n",
    "            x[:, 4:12], self.H2w[4:8], stride=2\n",
    "        )  # next 4 planes look at last 8 input planes\n",
    "        slice3 = F.conv2d(\n",
    "            torch.cat((x[:, 0:4], x[:, 8:12]), dim=1), self.H2w[8:12], stride=2\n",
    "        )  # last 4 planes are cross\n",
    "        x = torch.cat((slice1, slice2, slice3), dim=1) + self.H2b\n",
    "        x = torch.relu(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        # x is now shape (1, 12, 4, 4)\n",
    "        x = x.flatten(start_dim=1)  # (1, 12*4*4)\n",
    "        x = x @ self.H3w + self.H3b\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # x is now shape (1, 30)\n",
    "        x = x @ self.outw + self.outb\n",
    "\n",
    "        # x is finally shape (1, 10)\n",
    "        return x\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser(\n",
    "    #     description=\"Train a 2022 but mini ConvNet on digits\"\n",
    "    # )\n",
    "    # parser.add_argument(\n",
    "    #     \"--learning-rate\", \"-l\", type=float, default=3e-4, help=\"Learning rate\"\n",
    "    # )\n",
    "    # parser.add_argument(\n",
    "    #     \"--output-dir\",\n",
    "    #     \"-o\",\n",
    "    #     type=str,\n",
    "    #     default=\"out/modern\",\n",
    "    #     help=\"output directory for training logs\",\n",
    "    # )\n",
    "    # args = parser.parse_args()\n",
    "    # print(vars(args))\n",
    "    learning_rate = 3e-4\n",
    "    output_dir = \"out/modern\"\n",
    "\n",
    "    # init rng\n",
    "    torch.manual_seed(1337)\n",
    "    np.random.seed(1337)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    # set up logging\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # with open(os.path.join(args.output_dir, \"args.json\"), \"w\") as f:\n",
    "    #     json.dump(vars(args), f, indent=2)\n",
    "    writer = SummaryWriter(output_dir)\n",
    "\n",
    "    # init a model\n",
    "    model = Net()\n",
    "    print(\"model stats:\")\n",
    "    print(\n",
    "        \"# params:      \", sum(p.numel() for p in model.parameters())\n",
    "    )  # in paper total is 9,760\n",
    "    print(\"# MACs:        \", model.macs)\n",
    "    print(\"# activations: \", model.acts)\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "    # model = model.to(mps_device)\n",
    "    # init data\n",
    "    Xtr, Ytr = torch.load(\"train1989.pt\")\n",
    "    Xte, Yte = torch.load(\"test1989.pt\")\n",
    "\n",
    "    # init optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def eval_split(split):\n",
    "        # eval the full train/test set, batched implementation for efficiency\n",
    "        model.eval()\n",
    "        X, Y = (Xtr, Ytr) if split == \"train\" else (Xte, Yte)\n",
    "        Yhat = model(X)\n",
    "        loss = F.cross_entropy(yhat, y.argmax(dim=1))\n",
    "        err = torch.mean((Y.argmax(dim=1) != Yhat.argmax(dim=1)).float())\n",
    "        print(\n",
    "            f\"eval: split {split:5s}. loss {loss.item():e}. error {err.item()*100:.2f}%. misses: {int(err.item()*Y.size(0))}\"\n",
    "        )\n",
    "        writer.add_scalar(f\"error/{split}\", err.item() * 100, pass_num)\n",
    "        writer.add_scalar(f\"loss/{split}\", loss.item(), pass_num)\n",
    "\n",
    "    # train\n",
    "    for pass_num in range(80):\n",
    "        # learning rate decay\n",
    "        alpha = pass_num / 79\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = (1 - alpha) * learning_rate + alpha * (learning_rate / 3)\n",
    "\n",
    "        # perform one epoch of training\n",
    "        model.train()\n",
    "        for step_num in range(Xtr.size(0)):\n",
    "            # fetch a single example into a batch of 1\n",
    "            x, y = Xtr[[step_num]], Ytr[[step_num]]\n",
    "\n",
    "            # forward the model and the loss\n",
    "            yhat = model(x)\n",
    "            loss = F.cross_entropy(yhat, y.argmax(dim=1))\n",
    "\n",
    "            # calculate the gradient and update the parameters\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # after epoch epoch evaluate the train and test error / metrics\n",
    "        print(pass_num + 1)\n",
    "        eval_split(\"train\")\n",
    "        eval_split(\"test\")\n",
    "\n",
    "    # save final model to file\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Net' from 'repro' (/Users/tony/arc/pro/dev/pydev/ptrn-labs/.venv/lib/python3.12/site-packages/repro/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# load model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrepro\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Net\n\u001b[1;32m     17\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout/base\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m Net()\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Net' from 'repro' (/Users/tony/arc/pro/dev/pydev/ptrn-labs/.venv/lib/python3.12/site-packages/repro/__init__.py)"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# load model\n",
    "from repro import Net\n",
    "\n",
    "model_dir = \"out/base\"\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, \"model.pt\")))\n",
    "model.eval()\n",
    "# load data\n",
    "Xtr, Ytr = torch.load(\"train1989.pt\")\n",
    "Xte, Yte = torch.load(\"test1989.pt\")\n",
    "\n",
    "\n",
    "def grid_mistakes(X, Y):\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    ishow, nshow = 0, 14\n",
    "    for ix in range(X.size(0)):\n",
    "        x, y = X[[ix]], Y[[ix]]\n",
    "        yhat = model(x)\n",
    "        yi = y.argmax()\n",
    "        yhati = yhat.argmax()\n",
    "        if yi != yhati:\n",
    "            plt.subplot(2, 7, ishow + 1)\n",
    "            plt.imshow(x[0, 0], cmap=\"gray\")\n",
    "            plt.title(f\"gt={yi}, pred={yhati}\")\n",
    "            plt.axis(\"off\")\n",
    "            ishow += 1\n",
    "            if ishow >= nshow:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_mistakes(Xtr, Ytr)  # training set mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_mistakes(Xte, Yte)  # test set mistakes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
